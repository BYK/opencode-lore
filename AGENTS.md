<!-- This section is auto-maintained by lore (https://github.com/BYK/opencode-lore) -->
## Long-term Knowledge

### Architecture

<!-- lore:019c9f03-70c9-71b2-886e-c7f79fb51fcb -->
* **Release flow: Craft reusable workflow + accepted-label publish with OIDC npm**: The release process uses a two-workflow design: 1. \*\*release.yml\*\* (workflow\_dispatch): Delegates to \`getsentry/craft/.github/workflows/release.yml@v2\` with \`publish\_repo: self\`. Craft determines version from conventional commits, bumps package.json via \`preReleaseCommand: npm version $CRAFT\_NEW\_VERSION --no-git-tag-version\`, creates release branch + PR, and opens a publish issue \*\*in the same repo\*\* with target checkboxes and instructions to add the \`accepted\` label. 2. \*\*publish.yml\*\* (issues: labeled): Triggers when \`accepted\` label is added to an open issue. Parses version from issue title (\`publish: owner/repo@VERSION\`), runs \`craft publish\` for GitHub release/tag (only \`github\` target in .craft.yml), then \`npm publish --provenance --access public\` for npm OIDC trusted publishing. Closes issue on success; comments + removes label on failure. Key design decisions: - \`.craft.yml\` has only \`github\` target — avoids Craft's npm target which requires a tarball artifact and \`NPM\_TOKEN\`. npm publish runs separately with \`--provenance\` for OIDC. - \`id-token: write\` permission + \`setup-node\` with \`registry-url\` enables npm trusted publishing without any secret. - \`publish\_repo: self\` (Craft >= 2.22) creates the publish issue locally instead of in a shared getsentry/publish repo. - The \`production\` environment gate on the publish job provides an additional approval layer. - Caller workflow needs \`contents: write\`, \`pull-requests: write\`, \`issues: write\` permissions; reusable workflows use permission intersection.
<!-- lore:019c94bd-042d-73c0-b850-192d1d62fa68 -->
* **Knowledge entry distribution across projects — worktree sessions create separate project IDs**: Knowledge entries are scoped by project\_id, which is derived from ensureProject(projectPath). OpenCode worktree sessions have paths like ~/.local/share/opencode/worktree/\<hash>/\<session-slug>/, and each unique worktree path gets its own project\_id in the lore DB. This means a single actual project (e.g. opencode-lore) can have multiple project\_ids: one for the real path (/home/byk/Code/opencode-lore) and separate ones for each worktree session (witty-garden, glowing-cactus, etc.). Project-specific entries (cross\_project=0) created in a worktree session are invisible to the main project path and vice versa. Cross-project entries (cross\_project=1) are shared globally. Real data: opencode-lore's main path has 23 entries/~5335 tokens, but worktree sessions witty-garden (20 entries/~2962 tokens) and glowing-cactus (28 entries/~5688 tokens) have their own separate pools.
<!-- lore:019c94bd-042b-7215-b0a0-05719fcd39b2 -->
* **LTM injection pipeline: system transform → forSession → formatKnowledge → gradient deduction**: LTM knowledge is injected via the experimental.chat.system.transform hook in src/index.ts:333-376. Flow: (1) getLtmBudget() computes token ceiling as (contextLimit - outputReserved - overhead) \* ltmFraction (default 10%, configurable 2-30% in .lore.json budget.ltm). (2) ltm.forSession() loads project-specific + cross-project entries, scores cross-project by term overlap with session context (last distillation + last 10 temporal messages), greedy-packs into budget. (3) formatKnowledge() renders as markdown grouped by category, with its own secondary budget pass. (4) setLtmTokens() records actual consumption so the gradient transform deducts it from the message window budget (gradient.ts usable calculation subtracts ltmTokens). The LTM delta is also tracked in the exact-token calibration path. Key detail: LTM is injected into output.system (system prompt), not into the message array — so it's invisible to the gradient transform's tryFit() and counts against the overhead budget instead.
<!-- lore:019c91ad-4d49-7a08-bd93-782911d25e67 -->
* **OpenCode worktree-based session isolation**: OpenCode sessions operate in isolated worktrees at \`~/.local/share/opencode/worktree/\<projectHash>/\<session-slug>/\`. Each session gets a unique slug (e.g. 'witty-garden', 'gentle-falcon'). The session JSON in the \`session\` storage directory contains: id, slug, projectID (hash), directory (worktree path), title, version, summary (additions/deletions/files), and timestamps. The projectID is a hash of the project root directory. Test artifacts (.test-tmp/) accumulate in worktrees and can consume significant disk space.
<!-- lore:019c91ad-4d43-7507-8edc-2c8eaf4c1962 -->
* **OpenCode message storage: data column holds JSON, parts in separate table**: OpenCode stores messages in SQLite (\`~/.local/share/opencode/opencode.db\`) in a \`message\` table with columns: id, session\_id, time\_created, time\_updated, data. The \`data\` column contains the full message JSON including role, tokens, error, mode, etc. There is NO \`role\` column — role is nested inside the JSON. Message parts (text content, tool calls, step-start/finish) are stored in a separate \`part\` table with columns: id, message\_id, session\_id, time\_created, time\_updated, data. To query messages by role or mode, you must use JSON extraction functions (e.g. \`json\_extract(data, '$.role')\`). Compaction user messages often have empty text in the message data — their actual content (type:'compaction' or user text) is in the part table rows.
<!-- lore:019c904b-7922-76de-9a82-617c2edc6ad3 -->
* **ltm.create() dedup guard prevents same-title duplicate entries**: Before INSERT, ltm.create() checks for an existing row with the same project\_id + title (case-insensitive, confidence > 0). If found, calls update() with the new content instead of inserting a duplicate. This prevents the curator LLM from creating multiple entries for the same knowledge across sessions — the root cause of duplicate rows (e.g. 3 rows each for 'Lore temporal pruning' and 'Lore pruning defaults'). The guard is skipped when an explicit id is provided (cross-machine import via agents-file), since importFromFile handles dedup separately via UUID matching.
<!-- lore:019c904b-791e-772a-ab2b-93ac892a960c -->
* **buildSection() renders AGENTS.md directly without formatKnowledge**: In agents-file.ts, buildSection() now iterates DB entries directly grouped by category, emitting ### Category headings with \<!-- lore:UUID --> markers before each \* \*\*Title\*\*: Content bullet. This replaced the old approach of calling formatKnowledge() (remark AST serialization) then matching rendered bullets back to entries by title via a Map to inject UUID markers. The old title-keyed Map caused data loss when multiple entries shared the same title — Map.set() overwrote, giving all same-title bullets the same UUID. On re-import, seenIds skipped duplicates, silently losing entries. formatKnowledge() is now only used for system prompt injection where UUID tracking isn't needed. Individual entry content is serialized via remark for proper markdown escaping.
<!-- lore:019c8f8c-47c3-71a2-b5fd-248a2cfeba78 -->
* **Lore temporal pruning runs after distillation and curation on session.idle**: In src/index.ts, the session.idle handler now awaits both backgroundDistill and backgroundCurate (changed from fire-and-forget) before running temporal.prune(). This ordering is critical: pruning must run after both pipelines complete so it never deletes messages that haven't been processed. The prune call is wrapped in try/catch and logs when rows are deleted. Config comes from cfg.pruning.retention (days) and cfg.pruning.maxStorage (MB).
<!-- lore:019c8f4f-67c8-7cf4-b93b-c5ec46ed94b6 -->
* **Lore DB uses incremental auto\_vacuum to prevent free-page bloat**: The lore SQLite database (lore.db at ~/.local/share/opencode-lore/) accumulated 83% free pages from deletions — 428MB on disk with only 71MB of actual data. Fixed in schema version 3 migration: sets PRAGMA auto\_vacuum = INCREMENTAL then runs VACUUM. The migration is handled as a special case in migrate() since VACUUM can't run inside a transaction. The startup code also sets the pragma (no-op after migration, but ensures fresh installs get it). With incremental auto\_vacuum, freed pages return to the OS on each transaction commit instead of accumulating in the freelist. The temporal\_messages table (~51MB, ~6K rows) is the primary storage consumer; knowledge table is tiny (~74KB).
<!-- lore:20fe6f48-91de-4be3-bb29-a88e8238bd49 -->
* **AGENTS.md dedup via curator LLM at import time**: When multiple developers using lore merge branches, duplicate semantic entries can appear in AGENTS.md (different UUIDs, same meaning). Dedup is handled by the curator LLM during the import path — not as a separate compaction pass. The import sends parsed entries to the curator along with existing DB entries; the curator already has logic to produce update/delete ops when entries overlap semantically. This means dedup piggybacks on import, which runs at startup when the file has changed since last export. The import prompt is enhanced to explicitly ask: 'if any entries are semantically duplicative of existing entries, merge them.' For mangled \`\<!-- lore:UUID -->\` markers after merge conflict resolution: missing marker = treat as hand-written (new entry, curator handles dedup); duplicate same UUID = keep first, ignore second; malformed marker = ignore, treat as hand-written. Change detection uses content hash or mtime comparison of the lore section.
<!-- lore:019c8f95-c1a4-7b04-a575-23ebf89d14c3 -->
* **Lore gradient tryFit rebuilds message array from scratch every turn**: The gradient transform's tryFit() (gradient.ts) walks backwards from the end of messages, accumulating tokens until the raw budget is filled, then prepends distilled prefix messages. This produces a completely new array layout every turn — it doesn't track or preserve previous array positions. Combined with layer escalation (which strips tool outputs from previously-intact messages) and distillation updates (which change prefix content), the message array is never stable between consecutive turns at high utilization. This is the core incompatibility with prefix-based prompt caching. Critically, tryFit treats ALL messages as evictable, including messages from the current agentic turn. In tool-call loops, each step is a separate Message sharing the same parentID. As steps accumulate, their total tokens can push the cutoff forward past earlier steps in the same turn, causing the model to lose context of its own prior work. This is the root cause of infinite tool-call loops where the model repeats the same command. The fix requires protecting the current turn (last user message + all assistant messages sharing its parentID) as an atomic unit that tryFit never splits. As of v0.2.9, all gradient calibration state is per-session via \`Map\<sessionID, SessionState>\`, and worker sessions (distillation/curator) are skipped entirely in the transform hook.
<!-- lore:019c8f95-c19d-7269-90b3-5a5b0a43ee45 -->
* **Lore cache-preserving fix: Approach A (defer) then Approach B (stable prefix)**: Implemented fix for gradient transform cache invalidation — three layers, now layers 1+2 are shipped: \*\*Layer 0 — Approach A (passthrough):\*\* transform() estimates total message tokens first. If they fit within \`usable\` budget, returns \`{ messages: input.messages, layer: 0 }\` — same array reference, completely untouched. No DB reads, no prefix building, no tryFit(). In index.ts, the transform hook skips splice() and trailing-message check when layer === 0. This preserves append-only message patterns for prompt caching (~97% cache efficiency). Raw messages are strictly better context than lossy distilled summaries. \*\*Layer 1 — Approach C (append-only prefix cache):\*\* When gradient mode activates (context exhausted), \`distilledPrefixCached()\` tracks lastDistillationID, rowCount, and cachedText per session. Three paths: (1) no new rows → returns exact same prefixMessages object (byte-identical for cache); (2) new rows appended → renders only delta via formatDistillations(), appends to cachedText; (3) session change or meta-distillation rewrote rows → full re-render. \`buildPrefixMessages()\` extracted from old \`distilledPrefix()\` so both cached and non-cached paths share the message-wrapping logic. Layer 1 uses cached path; layers 2-4 use non-cached \`distilledPrefix()\` since they already invalidate cache via tool stripping. \*\*Layer 2 — Approach B (lazy eviction, not yet implemented):\*\* Track previous raw window cutoff by message ID, only advance when budget truly exceeded. SafetyLayer type is now \`0 | 1 | 2 | 3 | 4\`. \`resetPrefixCache()\` exported for testing.

### Decision

<!-- lore:019c904b-7924-7187-8471-8ad2423b8946 -->
* **Curator prompt scoped to code-relevant knowledge only**: CURATOR\_SYSTEM in src/prompt.ts now explicitly excludes: general ecosystem knowledge available online, business strategy and marketing positioning, product pricing models, third-party tool details not needed for development, and personal contact information. This was added after the curator extracted entries about OpenWork integration strategy (including an email address), Lore Cloud pricing tiers, and AGENTS.md ecosystem facts — none of which help an agent write code. The curatorUser() function also appends guidance to prefer updating existing entries over creating new ones for the same concept, reducing duplicate creation.
<!-- lore:019c8f8c-47c6-7e5a-8e93-7721dc1378dc -->
* **Lore pruning defaults: 120-day retention, 1GB size cap**: User chose 120 days retention (not the originally proposed 7 days) and 1GB max storage (not 200MB). Rationale: the system is new and 7 days/200MB was based on only one week of data from a one-week-old system — not indicative of real growth. The generous defaults preserve recall capability and historical context. Config is in .lore.json under pruning.retention (days) and pruning.maxStorage (MB).
<!-- lore:dd60622e-6cf3-48c7-9715-f44fb054e150 -->
* **Use uuidv7 npm package for knowledge entry IDs**: User chose the \`uuidv7\` npm package (https://npmx.dev/package/uuidv7, by LiosK) over a self-contained ~15 line implementation. The package is RFC 9562 compliant, provides \`uuidv7()\` function that returns standard UUID string format, has a 42-bit counter for sub-millisecond monotonic ordering, and is clean/minimal. Usage: \`import { uuidv7 } from 'uuidv7'; const id = uuidv7();\` replaces \`crypto.randomUUID()\` in \`ltm.ts:31\`. Added as a runtime dependency in package.json.

### Gotcha

<!-- lore:019c9f03-70cc-7e04-9835-e5c21f3d4e7d -->
* **Craft npm target requires tarball artifact — use github-only target for OIDC npm publish**: Craft's npm target expects a pre-built tarball (from \`npm pack\`) uploaded as a GitHub Actions artifact, and authenticates via \`NPM\_TOKEN\` secret. This is incompatible with npm OIDC trusted publishing (\`--provenance\`), which requires \`id-token: write\` permission and \`setup-node\` with \`registry-url\` — no token needed. Workaround: configure \`.craft.yml\` with only \`github\` target and an explicit \`preReleaseCommand\` for version bumping (e.g., \`npm version $CRAFT\_NEW\_VERSION --no-git-tag-version\`), then run \`npm publish --provenance --access public\` as a separate step after \`craft publish\`. Without the npm target, Craft's auto version bump won't touch package.json, so the preReleaseCommand is essential.
<!-- lore:019c9ee7-f55a-7ab0-9f4a-4147b5f535c2 -->
* **pnpm overrides become stale when dependency tree changes — audit with pnpm why**: pnpm overrides can become orphaned when the dependency tree changes. For example, removing a package that was the sole consumer of a transitive dep (like removing \`@sentry/typescript\` which pulled in \`tslint\` which was the only consumer of \`diff\`), or upgrading a package that switches to a differently-named dependency (like \`minimatch@10.2.4\` switching from \`@isaacs/brace-expansion\` to the unscoped \`brace-expansion\`). Orphaned overrides sit silently in package.json and could unexpectedly constrain versions if a future dependency reintroduces the package name. After removing packages or upgrading dependencies that change the transitive tree, audit overrides with \`pnpm why \<pkg>\` to verify each override still has consumers in the resolved tree. Remove any that return empty results.
<!-- lore:019c9eb7-a648-70f7-8a8d-5fc5b5c0f221 -->
* **git stash pop after merge can cause second conflict on same file**: When you stash local changes, merge, then \`git stash pop\`, the stash apply can create a NEW conflict on the same file that was just conflict-resolved in the merge. This happens because the stash was based on the pre-merge state and conflicts with the post-merge-resolution content. The resolution requires a second manual conflict resolution pass. To avoid: if stashed changes are lore/auto-generated content, consider just dropping the stash and re-running the generation tool after merge instead of popping.
<!-- lore:019c9eb7-a640-776a-929d-89120f81733a -->
* **pnpm-lock.yaml merge conflicts: regenerate don't manually merge**: When \`pnpm-lock.yaml\` has merge conflicts, never try to manually resolve the conflict markers. Instead: (1) \`git checkout --theirs pnpm-lock.yaml\` (or \`--ours\` depending on which package.json changes you want as base), (2) run \`pnpm install\` to regenerate the lockfile incorporating both sides' \`package.json\` changes (including overrides). This produces a clean lockfile that reflects the merged dependency state. Manual conflict resolution in lockfiles is error-prone and unnecessary since pnpm can regenerate it deterministically.
<!-- lore:019c9e9c-fa8f-7ab2-b26e-d47e50cb04bb -->
* **marked-terminal unconditionally imports cli-highlight and node-emoji — no tree-shaking possible**: marked-terminal has static top-level imports of \`cli-highlight\` (which pulls in highlight.js, ~570KB minified output) and \`node-emoji\` (which pulls in emojilib, ~208KB minified output) at lines 5-6 of its index.js. These are unconditional — there's no config option to disable them, and the \`emoji: false\` option only skips the emoji replacement function but doesn't prevent the import. esbuild cannot tree-shake static imports. This means any bundle including marked-terminal will grow by ~970KB (highlight.js + emojilib + parse5). To avoid this in a CLI bundle, you'd need to either: (1) write a custom marked renderer using only chalk, (2) fork marked-terminal with dynamic imports, or (3) use esbuild's \`external\` option (but then those packages must be available at runtime).
<!-- lore:019c9be1-33d1-7b6e-b107-ae7ad42a4ea4 -->
* **pnpm overrides with >= can cross major versions — use ^ to constrain**: When using pnpm overrides to patch a transitive dependency vulnerability, \`"ajv@<6.14.0": ">=6.14.0"\` will resolve to the latest ajv (v8.x), not the latest 6.x. ajv v6 and v8 have incompatible APIs — this broke eslint (\`@eslint/eslintrc\` calls \`ajv\` v6 API, crashes with \`Cannot set properties of undefined (setting 'defaultMeta')\` on v8). Fix: use \`"ajv@<6.14.0": "^6.14.0"\` to constrain within the same major. This applies to any override where the target package has multiple major versions in the registry — always use \`^\` (or \`~\`) instead of \`>=\` to stay within the compatible major line.
<!-- lore:019c9be1-33ca-714e-8ad9-dfda5350a106 -->
* **pnpm overrides with version-range keys don't force upgrades of already-compatible resolutions**: pnpm overrides with version-range selectors like \`"minimatch@>=10.0.0 <10.2.1": ">=10.2.1"\` do NOT work as expected for forcing upgrades of transitive deps that already satisfy their parent's semver range. If a parent requests \`^10.1.1\` and pnpm resolves \`10.1.1\`, the override key \`>=10.0.0 <10.2.1\` should match but doesn't reliably force re-resolution — even with \`pnpm install --force\`. The workaround is a blanket override without a version selector: \`"minimatch": ">=10.2.1"\`. This is only safe when ALL consumers are on the same major version line (otherwise it's a breaking change). Verify first with \`pnpm why \<pkg>\` that no other major versions exist in the tree before using a blanket override.
<!-- lore:019c9ba5-5158-77df-b32d-08980d0753c4 -->
* **git notes are lost on commit amend — must re-attach to new SHA**: Git notes are attached to a specific commit SHA. When you \`git commit --amend\`, the old commit is replaced with a new one (different SHA), and the note attached to the old SHA becomes orphaned. After amending, you must re-add the note to the new commit with \`git notes add\` targeting the new SHA. This also affects \`git push --force\` of notes refs — the remote note ref still points to the old SHA.
<!-- lore:019c91cc-0596-7379-9344-7c969695ecc0 -->
* **Trailing assistant messages after tryFit cause prefill API errors**: When the gradient transform (layers 1-4) produces a compressed message window, the last message may be a trailing assistant message. Anthropic's API requires the conversation to end with a user message. HOWEVER, an assistant message that has any \`tool\` parts (completed or pending) must NOT be dropped — OpenCode's SDK converts tool parts into \`tool\_result\` blocks sent as \`role: "user"\` messages at the API level, so these messages already end with a user-role message. Dropping them strips the entire current agentic turn and causes the model to restart from scratch in an infinite loop. Only pure-text assistant messages (zero tool parts) would actually cause a prefill error and should be dropped. Fix (v0.2.8): the drop predicate in src/index.ts changed from \`hasPendingTool\` (any non-completed tool) to \`hasToolParts\` (any tool at all) — stop dropping when a tool part is found, only drop pure-text trailing messages. Located in src/index.ts around lines 396-430. This was the actual root cause of the infinite tool-call loops at v0.2.5–v0.2.7 despite tryFit current-turn protection being correct.
<!-- lore:019c91d6-04af-7334-8374-e8bbf14cb43d -->
* **Calibration used DB message count instead of transformed window count — caused layer 0 false passthrough**: The gradient calibration stored \`lastKnownMessageCount\` from all DB messages (\`withParts.length\` from \`ctx.client.session.messages()\`), but \`lastKnownInput\` reflected only the compressed message window the model actually received (e.g. 50 msgs at layer 1). On the next turn, delta calculation was \`newMsgCount = totalDBMessages - lastKnownMessageCount = 431 - 430 = 1\`, so \`expectedInput ≈ 114K + ~50 tokens\` → layer 0 passthrough → all 431 uncompressed messages sent → 405K overflow. Fix (v0.2.3, commit 1cb90cc): \`transform()\` in gradient.ts now sets \`lastTransformedCount\` to \`result.messages.length\` after every call. The calibrate call in index.ts uses \`getLastTransformedCount()\` instead of \`withParts.length\`. Now after a compressed turn (50 msgs), \`lastKnownMessageCount=50\`, so next turn \`newMsgCount = 431 - 50 = 381\` → large delta → gradient activates correctly. Key insight: calibration's message count must match the \*\*transformed output\*\* (what was sent to the model), not the \*\*DB total\*\* (all session messages). These diverge whenever gradient layers 1-4 compress the window.
<!-- lore:019c91ea-ccff-76ff-a5af-ee12a066aabf -->
* **Gradient tryFit evicts mid-turn agentic steps — causes infinite tool-call loops**: The gradient's tryFit walks backwards from the end of messages, filling the raw budget. In agentic tool-call loops, each step is a separate OpenCode message (separate API call) sharing the same parentID as the user message. As the turn accumulates steps, the cumulative token count can exceed rawBudget, causing tryFit to set a cutoff that drops earlier steps from the same turn. The model then sees only the most recent step(s) + the user request, has no memory of prior work, and re-issues the same tool call — creating an infinite loop. Symptoms: (1) model repeats the exact same tool command across consecutive assistant messages, (2) total input tokens are stable (~150K) rather than growing, (3) cache.read fluctuates wildly between steps (24K vs 150K) because the window composition changes. Root cause: tryFit treats all messages equally — it doesn't distinguish the current agentic turn (which must be atomic) from older turns (which can be evicted). tryFitStable's pin mechanism doesn't help because it pins the oldest message in the window, and as new step messages are appended, the pinned window grows until it exceeds budget, triggering a fresh tryFit scan that picks a new cutoff mid-turn. Fix approach: identify the 'current turn' — the last user message plus all following assistant messages that share its parentID (available via msg.info.parentID). Protect these as an atomic unit in tryFit. Walk backwards as before but always include the current turn. Fill remaining budget with older messages. If the current turn alone exceeds rawBudget, return null to escalate to the next layer (which strips tool outputs, reducing size). The current turn should never be evictable.
<!-- lore:019c94bd-0428-7e8d-a1ae-e6d971bb767f -->
* **LTM forSession() gives project-specific entries unconditional priority — bloats prompt with irrelevant knowledge**: In src/ltm.ts:156-285, forSession() loads ALL project-specific entries (cross\_project=0, confidence>0.2) and gives them unconditional first-pick access to the token budget (lines 273-276) with no relevance scoring. Only cross-project entries get session-context-based relevance filtering (term overlap scoring, lines 214-253). For projects with 20+ knowledge entries (~5K tokens), this dumps the entire project knowledge base into every session's system prompt regardless of whether the entries are relevant to the current task. Combined with the default 10% LTM budget (~15K tokens), irrelevant project knowledge can consume a third of the LTM allocation. This contributed to prompt-too-long errors requiring manual compaction. Additionally, on first turn when no session context exists (no distillations, no temporal messages), the fallback takes top 10 cross-project entries by confidence alone (line 249) with zero relevance filtering. The term-overlap scoring itself is also coarse — common programming terms (function, return, const) cause unrelated entries to score positively. Proposed fix: apply relevance scoring to project entries too (with a lower threshold than cross-project), and cap the no-context fallback.
<!-- lore:019c91c0-cdf3-71c9-be52-7f6441fb643e -->
* **Lore plugin only protects projects where it's registered in opencode.json**: The lore gradient transform hook only runs for projects that have the lore plugin registered in their \`opencode.json\` config. OpenCode worktree sessions for projects without lore (no \`opencode.json\` or lore not listed in plugins) get zero context management — all messages accumulate uncompressed. If such a session exceeds the model's context limit, OpenCode's only response is built-in compaction which sends ALL messages as context, causing the same overflow in a stuck loop. This was the root cause of a 404K-token overflow in a getsentry/cli worktree session: no \`opencode.json\` existed, so the transform hook never fired. Prevention: ensure all actively-used projects have lore registered, or OpenCode itself needs a fallback context management strategy.
<!-- lore:019c91ad-4d47-7afc-90e0-239a9eda57a4 -->
* **Stuck compaction loops leave orphaned user+assistant message pairs in DB**: When OpenCode's compaction overflows (session too large for context limit), it creates paired user+assistant messages for each retry attempt. The assistant messages have \`error.name: 'ContextOverflowError'\` and \`mode: 'compaction'\`. The user messages have parts with \`type: 'compaction'\` or text like 'continue'/'go on'. These accumulate rapidly (17+ messages in one incident) and make the session worse. To recover a stuck session: (1) find the last good assistant message (has actual tokens, no error), (2) delete all messages after it — both assistant error/compaction messages AND their paired user messages, (3) also delete associated rows from the \`part\` table by message\_id. Use \`json\_extract(data, '$.error.name')\` and \`json\_extract(data, '$.mode')\` to identify compaction debris. The session will resume cleanly from the last good assistant message once the gradient correctly accounts for token usage.
<!-- lore:019c8ae9-2e57-7981-9bf3-8bbf6beaee31 -->
* **Gradient stats PATCH is non-critical — fail silently when server unreachable**: The lore plugin's gradient stats PATCH (metadata.lore on user message parts) was removed entirely — nothing in the codebase ever read the data back. This was the cleanest fix for the system-reminder persistence bug and also eliminated a fire-and-forget HTTP call every turn and the dependency on the private \_client SDK property. The lesson: don't mutate message parts you don't own. If stats display is needed in the future, use a proper mechanism (custom event type, dedicated endpoint) instead of piggy-backing on part mutation.
<!-- lore:096df3a6-0053-4026-960a-7196c80dcb2d -->
* **Lore gradient stats PATCH persists ephemeral system-reminder text**: The Lore gradient stats PATCH (metadata.lore on user message parts) was removed entirely. Nothing in the codebase ever read metadata.lore — it was write-only dead code. The PATCH was the root cause of the system-reminder persistence bug (PATCHing .text at all could persist ephemeral wrappers). Removal also eliminated: a fire-and-forget HTTP call on every turn, dependency on the private \_client SDK property, and the stripSystemReminders export from gradient.ts (reverted to module-private). The transform() return value still carries layer/distilledTokens/rawTokens for internal use (urgent distillation triggers, logging). If UI display is needed later, it should use a proper mechanism (custom event, dedicated endpoint) instead of piggy-backing on part mutation.
<!-- lore:019c8f4f-67ca-7212-a8c4-8a75b230ceea -->
* **Lore test suite uses live DB — no test isolation for db.test.ts**: The lore test suite (test/db.test.ts, test/ltm.test.ts) imports db() directly without setting LORE\_DB\_PATH, so it operates on the real database at ~/.local/share/opencode-lore/lore.db. This means: (1) migrations run against the live DB during tests, (2) test data (ensureProject for /test/project/alpha etc.) gets written to the real DB, (3) schema version assertions must match the current SCHEMA\_VERSION constant, (4) test fixtures in ltm.test.ts create knowledge entries with 019c9026-\* UUIDs that persist in the live DB and get exported to AGENTS.md — this caused 5 test-originated entries ('Kubernetes deployment pattern', 'TypeScript strict mode caveat', etc.) to appear in production AGENTS.md. A one-off cleanup deleted them. This is a known issue — tests aren't isolated from the live database.
<!-- lore:019c9986-fa44-7314-8380-3e451579f28b -->
* **Gradient module-level state shared across all sessions — worker sessions corrupt main session calibration**: All gradient calibration/transform state (lastLayer, lastKnownInput, lastWindowMessageIDs, lastKnownMessageCount, lastTransformedCount, forceMinLayer) was stored in module-level variables in gradient.ts. The transform hook in index.ts fires for ALL sessions including lore distillation and curator worker sessions. Worker sessions are small (13 messages), so their transform returns layer 0, overwriting lastLayer=0 and lastKnownSessionID to the worker session ID. When the main session's next step runs transform, calibrated=false (session ID mismatch) → sticky layer guard doesn't fire → layer 0 passthrough → 175K tokens. Next step gets calibrated back to the main session → compressed to 62K. This creates a stable oscillation: alternating 175K/62K every step. Confirmed with real data: 23 worker messages interleaved with 32 main session steps during oscillation. Fixed in v0.2.9: (1) replaced module-level vars with a \`Map\<sessionID, SessionState>\` so all calibration/tracking state is completely isolated per session, (2) added \`shouldSkip()\` guard to the \`messages.transform\` hook in index.ts that exits early for worker sessions (title contains 'lore distillation' or 'lore curator') before touching any state. DB persistence was considered unnecessary — UNCALIBRATED\_SAFETY=1.5 handles restarts. The \`lastWindowMessageIDs\` Set can contain 50-100 IDs per session and changes every step, making DB round-trips expensive for marginal restart-resilience benefit.
<!-- lore:019c91c0-cdf0-7a22-a8ce-950a03fd2237 -->
* **Gradient chars/4 estimate can undercount by 1.8x on first turn**: The gradient's \`estimate()\` function uses \`text.length / 4\` to approximate token counts. On first turn (no calibration data from previous API response), the entire message array is estimated this way. For sessions with 430+ messages containing tool calls, code blocks, and verbose output, the estimate was 226K while actual API tokenization was 404K — a 1.8x undercount. This matters because even though 226K > maxInput (168K) would trigger gradient mode, the gradient's tryFit() budget calculations are also based on chars/4 estimates, so compressed output may still exceed the real limit. The exact-tracking path (using \`lastKnownInput\` from previous API response + delta estimation) avoids this by only estimating the small delta of new messages. The first turn of a long-running session (after restart, cleanup, or session resume) is the vulnerable window. Critically, the undercount affects tryFit() itself: tryFit packs messages until chars/4 estimate fills the rawBudget (~107K), but actual tokenization of those messages is ~193K (1.8x). Combined with distilled prefix + overhead, this overflows the 200K limit. Layer 4 (emergency, last 3 messages) would fit, but layers 1-3 can all overflow because tryFit uses the same underestimating function for budget packing. Fix approach: apply a safety multiplier to tryFit output on uncalibrated first turns, or escalate to next layer if \`totalTokens \* safetyFactor > maxInput\`.
<!-- lore:019c9145-9658-7837-95e4-12e89e9272a8 -->
* **Gradient actualInput formula omitted cache.write — caused 380K overflow**: The gradient transform's calibration formula \`actualInput = msg.tokens.input + msg.tokens.cache.read\` missed \`cache.write\` tokens entirely. Cache.write tokens ARE sent to the model (they're being written to cache for the first time) but weren't counted. This caused the gradient to think the prompt was ~25K tokens when it was actually ~155K, keeping everything at layer 0 (passthrough). When cache went cold (cache.read=0, cache.write=154K), actualInput became 3, and the next turn sent all 475 raw messages → 407K tokens → ContextOverflowError. The guard condition \`(msg.tokens.input > 0 || msg.tokens.cache.read > 0)\` also skipped calibration on cold-cache turns. Fix (db1ec68): \`actualInput = msg.tokens.input + msg.tokens.cache.read + msg.tokens.cache.write\` and guard includes \`msg.tokens.cache.write > 0\`. Located in src/index.ts around line 191-213. Real-world confirmation: a stuck session showed last good assistant with input=3, cache.write=150107 — the gradient saw 3 tokens when the model processed 150K.

### Pattern

<!-- lore:019c9e9c-fa91-758c-8be9-f8ddb4e46eb5 -->
* **esbuild metafile output bytes vs input bytes — use output for real size impact**: When analyzing bundle size with esbuild's metafile, \`result.metafile.inputs\` shows raw source file sizes BEFORE minification and tree-shaking — these are misleading for size impact analysis. A 3.3MB input file may contribute 0 bytes to output if tree-shaken. Use \`result.metafile.outputs\[outfile].inputs\` to see actual per-file output contribution after minification. To dump metafile: add \`import { writeFileSync } from 'node:fs'; writeFileSync('/tmp/meta.json', JSON.stringify(result.metafile));\` after the build call, then analyze with \`jq\`. The bundle script at script/bundle.ts generates metafile but doesn't write it to disk by default.
<!-- lore:019c9bb9-a79b-71e0-9f71-d94e77119b4b -->
* **CLI UX: auto-correct common user mistakes with stderr warnings instead of hard errors**: When a CLI command can unambiguously detect a common user mistake (like using the wrong separator character), prefer auto-correcting the input and printing a warning to stderr over throwing a hard error. This is safe when: (1) the input is already invalid and would fail anyway, (2) there's no ambiguity in the correction, and (3) the warning goes to stderr so it doesn't interfere with JSON/stdout output. Implementation pattern: normalize inputs at the command level before passing to pure parsing functions, keeping the parsers side-effect-free. The \`gh\` CLI (GitHub CLI) is the UX model — match its conventions.
<!-- lore:019c8ae9-2e54-7276-966a-befe699db589 -->
* **Use SDK internal client for HTTP requests in OpenCode plugins**: OpenCode plugins should use \`(ctx.client as any).\_client.patch()\` (or \`.get()\`, \`.post()\`, etc.) instead of raw \`fetch()\` with \`ctx.serverUrl\`. The \`\_client\` property is the HeyAPI \`Client\` instance backing the SDK — it has the correct base URL, custom fetch handler, and interceptors already configured by the OpenCode runtime. This avoids ConnectionRefused errors in TUI-only mode where the HTTP server isn't listening. The internal client supports path interpolation: pass \`url: '/session/{sessionID}/message/{messageID}/part/{partID}'\` with a \`path: { sessionID, messageID, partID }\` object, and \`defaultPathSerializer\` handles template variable substitution. Downside: \`\_client\` is a private/undocumented property, so it could change in SDK updates. Note: lore's only use of this pattern (the stats PATCH) was removed because mutating message parts for metadata was the wrong approach — prefer proper mechanisms (custom events, dedicated endpoints) for plugin-to-UI communication.

### Preference

<!-- lore:019c9aa1-f7a2-7c42-b067-a87eff21df63 -->
* **General coding preference**: Prefer explicit error handling over silent failures
<!-- lore:019c9aa1-f75c-7cf4-921e-cc1d5fdccbe7 -->
* **Code style**: User prefers no backwards-compat shims, fix callers directly
<!-- End lore-managed section -->

## Conventions

* This project uses [Conventional Commits](https://www.conventionalcommits.org/) for all commit messages. Releases and changelogs are auto-generated from these via [Craft](https://github.com/getsentry/craft).
