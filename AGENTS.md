<!-- This section is auto-maintained by lore (https://github.com/BYK/opencode-lore) -->
## Long-term Knowledge

### Preference

<!-- lore:019c90e7-e045-7163-8e60-3f8e0cfabff0 -->
* **General coding preference**: Prefer explicit error handling over silent failures
<!-- lore:019c90e7-dff1-7cd2-9b1a-83f3c1a7a655 -->
* **Code style**: User prefers no backwards-compat shims, fix callers directly

### Pattern

<!-- lore:019c90e7-e03c-7fc1-b119-4214eb352aed -->
* **Kubernetes deployment pattern**: Use helm charts for Kubernetes deployments with resource limits
<!-- lore:019c8ae9-2e54-7276-966a-befe699db589 -->
* **Use SDK internal client for HTTP requests in OpenCode plugins**: OpenCode plugins should use \`(ctx.client as any).\_client.patch()\` (or \`.get()\`, \`.post()\`, etc.) instead of raw \`fetch()\` with \`ctx.serverUrl\`. The \`\_client\` property is the HeyAPI \`Client\` instance backing the SDK — it has the correct base URL, custom fetch handler, and interceptors already configured by the OpenCode runtime. This avoids ConnectionRefused errors in TUI-only mode where the HTTP server isn't listening. The internal client supports path interpolation: pass \`url: '/session/{sessionID}/message/{messageID}/part/{partID}'\` with a \`path: { sessionID, messageID, partID }\` object, and \`defaultPathSerializer\` handles template variable substitution. Downside: \`\_client\` is a private/undocumented property, so it could change in SDK updates.

### Gotcha

<!-- lore:019c90e7-e03a-7e0b-a21c-438a18455692 -->
* **TypeScript strict mode caveat**: TypeScript strict null checks require explicit undefined handling
<!-- lore:019c8f4f-67ca-7212-a8c4-8a75b230ceea -->
* **Lore test suite uses live DB — no test isolation for db.test.ts**: The lore test suite (test/db.test.ts, test/ltm.test.ts) imports db() directly without setting LORE\_DB\_PATH, so it operates on the real database at ~/.local/share/opencode-lore/lore.db. This means: (1) migrations run against the live DB during tests, (2) test data (ensureProject for /test/project/alpha etc.) gets written to the real DB, (3) schema version assertions must match the current SCHEMA\_VERSION constant, (4) test fixtures in ltm.test.ts create knowledge entries with 019c9026-\* UUIDs that persist in the live DB and get exported to AGENTS.md — this caused 5 test-originated entries ('Kubernetes deployment pattern', 'TypeScript strict mode caveat', etc.) to appear in production AGENTS.md. A one-off cleanup deleted them. This is a known issue — tests aren't isolated from the live database.
<!-- lore:019c8ae9-2e57-7981-9bf3-8bbf6beaee31 -->
* **Gradient stats PATCH is non-critical — fail silently when server unreachable**: The lore plugin's gradient stats metadata (layer, distilledTokens, rawTokens, budgets, updatedAt) PATCHed onto user message parts is purely for UI display — lore never reads it back. In TUI-only mode the HTTP server doesn't listen, so PATCH calls fail with ConnectionRefused. The .catch() handler should be silent (no console.error) to avoid spamming terminal output on every message. This was fixed by switching from raw fetch() to the SDK internal client and using an empty .catch().
<!-- lore:096df3a6-0053-4026-960a-7196c80dcb2d -->
* **Lore gradient stats PATCH persists ephemeral system-reminder text**: The Lore gradient stats PATCH ephemeral system-reminder persistence bug has been fixed. The statsPart lookup now happens AFTER the splice (when layer > 0) or references the original unmodified messages (layer 0, which are never wrapped with system-reminders since no transform runs). The comment in index.ts documents the fix: 'Look up statsPart AFTER the transform so the PATCHed text is clean (system-reminder wrappers stripped). Looking up before would persist ephemeral system-reminder content, making it visible in the UI.' The PATCH still sends the full part object including .text — partial metadata-only updates were not pursued.

### Decision

<!-- lore:019c904b-7924-7187-8471-8ad2423b8946 -->
* **Curator prompt scoped to code-relevant knowledge only**: CURATOR\_SYSTEM in src/prompt.ts now explicitly excludes: general ecosystem knowledge available online, business strategy and marketing positioning, product pricing models, third-party tool details not needed for development, and personal contact information. This was added after the curator extracted entries about OpenWork integration strategy (including an email address), Lore Cloud pricing tiers, and AGENTS.md ecosystem facts — none of which help an agent write code. The curatorUser() function also appends guidance to prefer updating existing entries over creating new ones for the same concept, reducing duplicate creation.
<!-- lore:019c8f8c-47c6-7e5a-8e93-7721dc1378dc -->
* **Lore pruning defaults: 120-day retention, 1GB size cap**: User chose 120 days retention (not the originally proposed 7 days) and 1GB max storage (not 200MB). Rationale: the system is new and 7 days/200MB was based on only one week of data from a one-week-old system — not indicative of real growth. The generous defaults preserve recall capability and historical context. Config is in .lore.json under pruning.retention (days) and pruning.maxStorage (MB).
<!-- lore:dd60622e-6cf3-48c7-9715-f44fb054e150 -->
* **Use uuidv7 npm package for knowledge entry IDs**: User chose the \`uuidv7\` npm package (https://npmx.dev/package/uuidv7, by LiosK) over a self-contained ~15 line implementation. The package is RFC 9562 compliant, provides \`uuidv7()\` function that returns standard UUID string format, has a 42-bit counter for sub-millisecond monotonic ordering, and is clean/minimal. Usage: \`import { uuidv7 } from 'uuidv7'; const id = uuidv7();\` replaces \`crypto.randomUUID()\` in \`ltm.ts:31\`. Added as a runtime dependency in package.json.

### Architecture

<!-- lore:019c904b-7922-76de-9a82-617c2edc6ad3 -->
* **ltm.create() dedup guard prevents same-title duplicate entries**: Before INSERT, ltm.create() checks for an existing row with the same project\_id + title (case-insensitive, confidence > 0). If found, calls update() with the new content instead of inserting a duplicate. This prevents the curator LLM from creating multiple entries for the same knowledge across sessions — the root cause of duplicate rows (e.g. 3 rows each for 'Lore temporal pruning' and 'Lore pruning defaults'). The guard is skipped when an explicit id is provided (cross-machine import via agents-file), since importFromFile handles dedup separately via UUID matching.
<!-- lore:019c904b-791e-772a-ab2b-93ac892a960c -->
* **buildSection() renders AGENTS.md directly without formatKnowledge**: In agents-file.ts, buildSection() now iterates DB entries directly grouped by category, emitting ### Category headings with \<!-- lore:UUID --> markers before each \* \*\*Title\*\*: Content bullet. This replaced the old approach of calling formatKnowledge() (remark AST serialization) then matching rendered bullets back to entries by title via a Map to inject UUID markers. The old title-keyed Map caused data loss when multiple entries shared the same title — Map.set() overwrote, giving all same-title bullets the same UUID. On re-import, seenIds skipped duplicates, silently losing entries. formatKnowledge() is now only used for system prompt injection where UUID tracking isn't needed. Individual entry content is serialized via remark for proper markdown escaping.
<!-- lore:019c8f95-c1a4-7b04-a575-23ebf89d14c3 -->
* **Lore gradient tryFit rebuilds message array from scratch every turn**: The gradient transform's tryFit() (gradient.ts:512-571) walks backwards from the end of messages, accumulating tokens until the raw budget is filled, then prepends distilled prefix messages. This produces a completely new array layout every turn — it doesn't track or preserve previous array positions. Combined with layer escalation (which strips tool outputs from previously-intact messages) and distillation updates (which change prefix content), the message array is never stable between consecutive turns at high utilization. This is the core incompatibility with prefix-based prompt caching.
<!-- lore:019c8f95-c1a2-7418-b577-c6487de50442 -->
* **Anthropic prompt caching requires byte-identical prefix stability**: Anthropic caches the longest matching prefix of the prompt. OpenCode places up to 4 cache\_control:ephemeral breakpoints (provider/transform.ts:165-199): first 2 system messages + last 2 conversation messages. Cache hits require byte-identical content from prompt start to a previously cached breakpoint. Any change at position N invalidates everything from N onwards, regardless of whether later content is identical. This means any plugin that modifies, reorders, or replaces messages in the middle of the conversation array will break caching for all subsequent messages. Append-only message patterns are cache-optimal. The system prompt cache is the most valuable — making it larger or more volatile is counterproductive.
<!-- lore:019c8f8c-47c3-71a2-b5fd-248a2cfeba78 -->
* **Lore temporal pruning runs after distillation and curation on session.idle**: In src/index.ts, the session.idle handler now awaits both backgroundDistill and backgroundCurate (changed from fire-and-forget) before running temporal.prune(). This ordering is critical: pruning must run after both pipelines complete so it never deletes messages that haven't been processed. The prune call is wrapped in try/catch and logs when rows are deleted. Config comes from cfg.pruning.retention (days) and cfg.pruning.maxStorage (MB).
* **Lore DB uses incremental auto\_vacuum to prevent free-page bloat**: The lore SQLite database (lore.db at ~/.local/share/opencode-lore/) accumulated 83% free pages from deletions — 428MB on disk with only 71MB of actual data. Fixed in schema version 3 migration: sets PRAGMA auto\_vacuum = INCREMENTAL then runs VACUUM. The migration is handled as a special case in migrate() since VACUUM can't run inside a transaction. The startup code also sets the pragma (no-op after migration, but ensures fresh installs get it). With incremental auto\_vacuum, freed pages return to the OS on each transaction commit instead of accumulating in the freelist. The temporal\_messages table (~51MB, ~6K rows) is the primary storage consumer; knowledge table is tiny (~74KB).
<!-- lore:20fe6f48-91de-4be3-bb29-a88e8238bd49 -->
* **AGENTS.md dedup via curator LLM at import time**: When multiple developers using lore merge branches, duplicate semantic entries can appear in AGENTS.md (different UUIDs, same meaning). Dedup is handled by the curator LLM during the import path — not as a separate compaction pass. The import sends parsed entries to the curator along with existing DB entries; the curator already has logic to produce update/delete ops when entries overlap semantically. This means dedup piggybacks on import, which runs at startup when the file has changed since last export. The import prompt is enhanced to explicitly ask: 'if any entries are semantically duplicative of existing entries, merge them.' For mangled \`\<!-- lore:UUID -->\` markers after merge conflict resolution: missing marker = treat as hand-written (new entry, curator handles dedup); duplicate same UUID = keep first, ignore second; malformed marker = ignore, treat as hand-written. Change detection uses content hash or mtime comparison of the lore section.
<!-- lore:019c8f95-c19d-7269-90b3-5a5b0a43ee45 -->
* **Lore cache-preserving fix: Approach A (defer) then Approach B (stable prefix)**: Implemented fix for gradient transform cache invalidation — three layers, now layers 1+2 are shipped: \*\*Layer 0 — Approach A (passthrough):\*\* transform() estimates total message tokens first. If they fit within \`usable\` budget, returns \`{ messages: input.messages, layer: 0 }\` — same array reference, completely untouched. No DB reads, no prefix building, no tryFit(). In index.ts, the transform hook skips splice() and trailing-message check when layer === 0. This preserves append-only message patterns for prompt caching (~97% cache efficiency). Raw messages are strictly better context than lossy distilled summaries. \*\*Layer 1 — Approach C (append-only prefix cache):\*\* When gradient mode activates (context exhausted), \`distilledPrefixCached()\` tracks lastDistillationID, rowCount, and cachedText per session. Three paths: (1) no new rows → returns exact same prefixMessages object (byte-identical for cache); (2) new rows appended → renders only delta via formatDistillations(), appends to cachedText; (3) session change or meta-distillation rewrote rows → full re-render. \`buildPrefixMessages()\` extracted from old \`distilledPrefix()\` so both cached and non-cached paths share the message-wrapping logic. Layer 1 uses cached path; layers 2-4 use non-cached \`distilledPrefix()\` since they already invalidate cache via tool stripping. \*\*Layer 2 — Approach B (lazy eviction, not yet implemented):\*\* Track previous raw window cutoff by message ID, only advance when budget truly exceeded. SafetyLayer type is now \`0 | 1 | 2 | 3 | 4\`. \`resetPrefixCache()\` exported for testing.

<!-- End lore-managed section -->
