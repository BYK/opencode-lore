<!-- This section is auto-maintained by lore (https://github.com/BYK/opencode-lore) -->
## Long-term Knowledge

### Gotcha

<!-- lore:019c91d6-04af-7334-8374-e8bbf14cb43d -->
* **Calibration used DB message count instead of transformed window count — caused layer 0 false passthrough**: The gradient calibration stored \`lastKnownMessageCount\` from all DB messages (\`withParts.length\` from \`ctx.client.session.messages()\`), but \`lastKnownInput\` reflected only the compressed message window the model actually received (e.g. 50 msgs at layer 1). On the next turn, delta calculation was \`newMsgCount = totalDBMessages - lastKnownMessageCount = 431 - 430 = 1\`, so \`expectedInput ≈ 114K + ~50 tokens\` → layer 0 passthrough → all 431 uncompressed messages sent → 405K overflow. Fix (v0.2.3, commit 1cb90cc): \`transform()\` in gradient.ts now sets \`lastTransformedCount\` to \`result.messages.length\` after every call. The calibrate call in index.ts uses \`getLastTransformedCount()\` instead of \`withParts.length\`. Now after a compressed turn (50 msgs), \`lastKnownMessageCount=50\`, so next turn \`newMsgCount = 431 - 50 = 381\` → large delta → gradient activates correctly. Key insight: calibration's message count must match the \*\*transformed output\*\* (what was sent to the model), not the \*\*DB total\*\* (all session messages). These diverge whenever gradient layers 1-4 compress the window.
<!-- lore:019c91ea-ccff-76ff-a5af-ee12a066aabf -->
* **Gradient tryFit evicts mid-turn agentic steps — causes infinite tool-call loops**: The gradient's tryFit walks backwards from the end of messages, filling the raw budget. In agentic tool-call loops, each step is a separate OpenCode message (separate API call) sharing the same parentID as the user message. As the turn accumulates steps, the cumulative token count can exceed rawBudget, causing tryFit to set a cutoff that drops earlier steps from the same turn. The model then sees only the most recent step(s) + the user request, has no memory of prior work, and re-issues the same tool call — creating an infinite loop. Symptoms: (1) model repeats the exact same tool command across consecutive assistant messages, (2) total input tokens are stable (~150K) rather than growing, (3) cache.read fluctuates wildly between steps (24K vs 150K) because the window composition changes. Root cause: tryFit treats all messages equally — it doesn't distinguish the current agentic turn (which must be atomic) from older turns (which can be evicted). tryFitStable's pin mechanism doesn't help because it pins the oldest message in the window, and as new step messages are appended, the pinned window grows until it exceeds budget, triggering a fresh tryFit scan that picks a new cutoff mid-turn. Fix approach: identify the 'current turn' — the last user message plus all following assistant messages that share its parentID (available via msg.info.parentID). Protect these as an atomic unit in tryFit. Walk backwards as before but always include the current turn. Fill remaining budget with older messages. If the current turn alone exceeds rawBudget, return null to escalate to the next layer (which strips tool outputs, reducing size). The current turn should never be evictable.
<!-- lore:019c94f0-2ab4-74b2-8bfa-d3ddfbb97d70 -->
* **GitHub Actions: use deterministic timestamps across jobs, not Date.now()**: When multiple GitHub Actions jobs need to agree on a timestamp-based version string (e.g., nightly builds where \`build-binary\` bakes the version in and \`publish-nightly\` creates version.json), never use \`Date.now()\` or \`Math.floor(Date.now()/1000)\` independently in each job. Jobs run at different times, producing different timestamps. Instead, derive the timestamp from a shared deterministic source like \`github.event.head\_commit.timestamp\` (the commit time). Convert to Unix seconds: \`date -d '\<iso-timestamp>' +%s\`. This ensures all matrix entries and downstream jobs produce the same version string.
<!-- lore:019c94d8-330e-71c0-a2ed-1d4b7a7cedee -->
* **React useState async pitfall**: React useState setter is async — reading state immediately after setState returns stale value in dashboard components
<!-- lore:019c94d8-32e0-7b74-88d7-1c91bafc9956 -->
* **TypeScript strict mode caveat**: TypeScript strict null checks require explicit undefined handling
<!-- lore:019c94bd-0428-7e8d-a1ae-e6d971bb767f -->
* **LTM forSession() gives project-specific entries unconditional priority — bloats prompt with irrelevant knowledge**: In src/ltm.ts:156-285, forSession() loads ALL project-specific entries (cross\_project=0, confidence>0.2) and gives them unconditional first-pick access to the token budget (lines 273-276) with no relevance scoring. Only cross-project entries get session-context-based relevance filtering (term overlap scoring, lines 214-253). For projects with 20+ knowledge entries (~5K tokens), this dumps the entire project knowledge base into every session's system prompt regardless of whether the entries are relevant to the current task. Combined with the default 10% LTM budget (~15K tokens), irrelevant project knowledge can consume a third of the LTM allocation. This contributed to prompt-too-long errors requiring manual compaction. Additionally, on first turn when no session context exists (no distillations, no temporal messages), the fallback takes top 10 cross-project entries by confidence alone (line 249) with zero relevance filtering. The term-overlap scoring itself is also coarse — common programming terms (function, return, const) cause unrelated entries to score positively. Proposed fix: apply relevance scoring to project entries too (with a lower threshold than cross-project), and cap the no-context fallback.
<!-- lore:019c91cc-0596-7379-9344-7c969695ecc0 -->
* **Trailing assistant messages after tryFit cause prefill API errors — but tool-bearing ones must NOT be dropped**: When the gradient transform (layers 1-4) produces a compressed message window, the last message may be a trailing assistant message. Anthropic's API requires the conversation to end with a user message. HOWEVER, an assistant message that has any \`tool\` parts (completed or pending) must NOT be dropped — OpenCode's SDK converts tool parts into \`tool_result\` blocks sent as \`role: "user"\` messages at the API level, so these messages already end with a user-role message. Dropping them strips the entire current agentic turn and causes the model to restart from scratch in an infinite loop. Only pure-text assistant messages (zero tool parts) would actually cause a prefill error and should be dropped. Fix (v0.2.8): the drop predicate in src/index.ts changed from \`hasPendingTool\` (any non-completed tool) to \`hasToolParts\` (any tool at all) — stop dropping when a tool part is found, only drop pure-text trailing messages. Located in src/index.ts around lines 396-430. This was the actual root cause of the infinite tool-call loops at v0.2.5–v0.2.7 despite tryFit current-turn protection being correct.
<!-- lore:019c91c0-cdf3-71c9-be52-7f6441fb643e -->
* **Lore plugin only protects projects where it's registered in opencode.json**: The lore gradient transform hook only runs for projects that have the lore plugin registered in their \`opencode.json\` config. OpenCode worktree sessions for projects without lore (no \`opencode.json\` or lore not listed in plugins) get zero context management — all messages accumulate uncompressed. If such a session exceeds the model's context limit, OpenCode's only response is built-in compaction which sends ALL messages as context, causing the same overflow in a stuck loop. This was the root cause of a 404K-token overflow in a getsentry/cli worktree session: no \`opencode.json\` existed, so the transform hook never fired. Prevention: ensure all actively-used projects have lore registered, or OpenCode itself needs a fallback context management strategy.
<!-- lore:019c91ad-4d47-7afc-90e0-239a9eda57a4 -->
* **Stuck compaction loops leave orphaned user+assistant message pairs in DB**: When OpenCode's compaction overflows (session too large for context limit), it creates paired user+assistant messages for each retry attempt. The assistant messages have \`error.name: 'ContextOverflowError'\` and \`mode: 'compaction'\`. The user messages have parts with \`type: 'compaction'\` or text like 'continue'/'go on'. These accumulate rapidly (17+ messages in one incident) and make the session worse. To recover a stuck session: (1) find the last good assistant message (has actual tokens, no error), (2) delete all messages after it — both assistant error/compaction messages AND their paired user messages, (3) also delete associated rows from the \`part\` table by message\_id. Use \`json\_extract(data, '$.error.name')\` and \`json\_extract(data, '$.mode')\` to identify compaction debris. The session will resume cleanly from the last good assistant message once the gradient correctly accounts for token usage.
<!-- lore:019c8ae9-2e57-7981-9bf3-8bbf6beaee31 -->
* **Gradient stats PATCH is non-critical — fail silently when server unreachable**: The lore plugin's gradient stats PATCH (metadata.lore on user message parts) was removed entirely — nothing in the codebase ever read the data back. This was the cleanest fix for the system-reminder persistence bug and also eliminated a fire-and-forget HTTP call every turn and the dependency on the private \_client SDK property. The lesson: don't mutate message parts you don't own. If stats display is needed in the future, use a proper mechanism (custom event type, dedicated endpoint) instead of piggy-backing on part mutation.
<!-- lore:096df3a6-0053-4026-960a-7196c80dcb2d -->
* **Lore gradient stats PATCH persists ephemeral system-reminder text**: The Lore gradient stats PATCH (metadata.lore on user message parts) was removed entirely. Nothing in the codebase ever read metadata.lore — it was write-only dead code. The PATCH was the root cause of the system-reminder persistence bug (PATCHing .text at all could persist ephemeral wrappers). Removal also eliminated: a fire-and-forget HTTP call on every turn, dependency on the private \_client SDK property, and the stripSystemReminders export from gradient.ts (reverted to module-private). The transform() return value still carries layer/distilledTokens/rawTokens for internal use (urgent distillation triggers, logging). If UI display is needed later, it should use a proper mechanism (custom event, dedicated endpoint) instead of piggy-backing on part mutation.
<!-- lore:019c8f4f-67ca-7212-a8c4-8a75b230ceea -->
* **Lore test suite uses live DB — no test isolation for db.test.ts**: The lore test suite (test/db.test.ts, test/ltm.test.ts) imports db() directly without setting LORE\_DB\_PATH, so it operates on the real database at ~/.local/share/opencode-lore/lore.db. This means: (1) migrations run against the live DB during tests, (2) test data (ensureProject for /test/project/alpha etc.) gets written to the real DB, (3) schema version assertions must match the current SCHEMA\_VERSION constant, (4) test fixtures in ltm.test.ts create knowledge entries with 019c9026-\* UUIDs that persist in the live DB and get exported to AGENTS.md — this caused 5 test-originated entries ('Kubernetes deployment pattern', 'TypeScript strict mode caveat', etc.) to appear in production AGENTS.md. A one-off cleanup deleted them. This is a known issue — tests aren't isolated from the live database.
<!-- lore:019c91c0-cdf0-7a22-a8ce-950a03fd2237 -->
* **Gradient chars/4 estimate can undercount by 1.8x on first turn**: The gradient's \`estimate()\` function uses \`text.length / 4\` to approximate token counts. On first turn (no calibration data from previous API response), the entire message array is estimated this way. For sessions with 430+ messages containing tool calls, code blocks, and verbose output, the estimate was 226K while actual API tokenization was 404K — a 1.8x undercount. This matters because even though 226K > maxInput (168K) would trigger gradient mode, the gradient's tryFit() budget calculations are also based on chars/4 estimates, so compressed output may still exceed the real limit. The exact-tracking path (using \`lastKnownInput\` from previous API response + delta estimation) avoids this by only estimating the small delta of new messages. The first turn of a long-running session (after restart, cleanup, or session resume) is the vulnerable window. Critically, the undercount affects tryFit() itself: tryFit packs messages until chars/4 estimate fills the rawBudget (~107K), but actual tokenization of those messages is ~193K (1.8x). Combined with distilled prefix + overhead, this overflows the 200K limit. Layer 4 (emergency, last 3 messages) would fit, but layers 1-3 can all overflow because tryFit uses the same underestimating function for budget packing. Fix approach: apply a safety multiplier to tryFit output on uncalibrated first turns, or escalate to next layer if \`totalTokens \* safetyFactor > maxInput\`.
<!-- lore:019c9145-9658-7837-95e4-12e89e9272a8 -->
* **Gradient actualInput formula omitted cache.write — caused 380K overflow**: The gradient transform's calibration formula \`actualInput = msg.tokens.input + msg.tokens.cache.read\` missed \`cache.write\` tokens entirely. Cache.write tokens ARE sent to the model (they're being written to cache for the first time) but weren't counted. This caused the gradient to think the prompt was ~25K tokens when it was actually ~155K, keeping everything at layer 0 (passthrough). When cache went cold (cache.read=0, cache.write=154K), actualInput became 3, and the next turn sent all 475 raw messages → 407K tokens → ContextOverflowError. The guard condition \`(msg.tokens.input > 0 || msg.tokens.cache.read > 0)\` also skipped calibration on cold-cache turns. Fix (db1ec68): \`actualInput = msg.tokens.input + msg.tokens.cache.read + msg.tokens.cache.write\` and guard includes \`msg.tokens.cache.write > 0\`. Located in src/index.ts around line 191-213. Real-world confirmation: a stuck session showed last good assistant with input=3, cache.write=150107 — the gradient saw 3 tokens when the model processed 150K.

### Preference

<!-- lore:019c94d8-32e9-7a5e-b9ad-80c7727ab9ff -->
* **General coding preference**: Prefer explicit error handling over silent failures
<!-- lore:019c94d8-3282-7dc4-b9d9-92f8ccbf2686 -->
* **Code style**: User prefers no backwards-compat shims, fix callers directly

### Pattern

<!-- lore:019c94d8-32e1-714f-9d35-887749abd08b -->
* **Kubernetes deployment pattern**: Use helm charts for Kubernetes deployments with resource limits
<!-- lore:019c8ae9-2e54-7276-966a-befe699db589 -->
* **Use SDK internal client for HTTP requests in OpenCode plugins**: OpenCode plugins should use \`(ctx.client as any).\_client.patch()\` (or \`.get()\`, \`.post()\`, etc.) instead of raw \`fetch()\` with \`ctx.serverUrl\`. The \`\_client\` property is the HeyAPI \`Client\` instance backing the SDK — it has the correct base URL, custom fetch handler, and interceptors already configured by the OpenCode runtime. This avoids ConnectionRefused errors in TUI-only mode where the HTTP server isn't listening. The internal client supports path interpolation: pass \`url: '/session/{sessionID}/message/{messageID}/part/{partID}'\` with a \`path: { sessionID, messageID, partID }\` object, and \`defaultPathSerializer\` handles template variable substitution. Downside: \`\_client\` is a private/undocumented property, so it could change in SDK updates. Note: lore's only use of this pattern (the stats PATCH) was removed because mutating message parts for metadata was the wrong approach — prefer proper mechanisms (custom events, dedicated endpoints) for plugin-to-UI communication.

### Architecture

<!-- lore:019c94bd-042d-73c0-b850-192d1d62fa68 -->
* **Knowledge entry distribution across projects — worktree sessions create separate project IDs**: Knowledge entries are scoped by project\_id, which is derived from ensureProject(projectPath). OpenCode worktree sessions have paths like ~/.local/share/opencode/worktree/\<hash>/\<session-slug>/, and each unique worktree path gets its own project\_id in the lore DB. This means a single actual project (e.g. opencode-lore) can have multiple project\_ids: one for the real path (/home/byk/Code/opencode-lore) and separate ones for each worktree session (witty-garden, glowing-cactus, etc.). Project-specific entries (cross\_project=0) created in a worktree session are invisible to the main project path and vice versa. Cross-project entries (cross\_project=1) are shared globally. Real data: opencode-lore's main path has 23 entries/~5335 tokens, but worktree sessions witty-garden (20 entries/~2962 tokens) and glowing-cactus (28 entries/~5688 tokens) have their own separate pools.
<!-- lore:019c94bd-042b-7215-b0a0-05719fcd39b2 -->
* **LTM injection pipeline: system transform → forSession → formatKnowledge → gradient deduction**: LTM knowledge is injected via the experimental.chat.system.transform hook in src/index.ts:333-376. Flow: (1) getLtmBudget() computes token ceiling as (contextLimit - outputReserved - overhead) \* ltmFraction (default 10%, configurable 2-30% in .lore.json budget.ltm). (2) ltm.forSession() loads project-specific + cross-project entries, scores cross-project by term overlap with session context (last distillation + last 10 temporal messages), greedy-packs into budget. (3) formatKnowledge() renders as markdown grouped by category, with its own secondary budget pass. (4) setLtmTokens() records actual consumption so the gradient transform deducts it from the message window budget (gradient.ts usable calculation subtracts ltmTokens). The LTM delta is also tracked in the exact-token calibration path. Key detail: LTM is injected into output.system (system prompt), not into the message array — so it's invisible to the gradient transform's tryFit() and counts against the overhead budget instead.
<!-- lore:019c91ad-4d49-7a08-bd93-782911d25e67 -->
* **OpenCode worktree-based session isolation**: OpenCode sessions operate in isolated worktrees at \`~/.local/share/opencode/worktree/\<projectHash>/\<session-slug>/\`. Each session gets a unique slug (e.g. 'witty-garden', 'gentle-falcon'). The session JSON in the \`session\` storage directory contains: id, slug, projectID (hash), directory (worktree path), title, version, summary (additions/deletions/files), and timestamps. The projectID is a hash of the project root directory. Test artifacts (.test-tmp/) accumulate in worktrees and can consume significant disk space.
<!-- lore:019c91ad-4d43-7507-8edc-2c8eaf4c1962 -->
* **OpenCode message storage: data column holds JSON, parts in separate table**: OpenCode stores messages in SQLite (\`~/.local/share/opencode/opencode.db\`) in a \`message\` table with columns: id, session\_id, time\_created, time\_updated, data. The \`data\` column contains the full message JSON including role, tokens, error, mode, etc. There is NO \`role\` column — role is nested inside the JSON. Message parts (text content, tool calls, step-start/finish) are stored in a separate \`part\` table with columns: id, message\_id, session\_id, time\_created, time\_updated, data. To query messages by role or mode, you must use JSON extraction functions (e.g. \`json\_extract(data, '$.role')\`). Compaction user messages often have empty text in the message data — their actual content (type:'compaction' or user text) is in the part table rows.
<!-- lore:019c904b-7922-76de-9a82-617c2edc6ad3 -->
* **ltm.create() dedup guard prevents same-title duplicate entries**: Before INSERT, ltm.create() checks for an existing row with the same project\_id + title (case-insensitive, confidence > 0). If found, calls update() with the new content instead of inserting a duplicate. This prevents the curator LLM from creating multiple entries for the same knowledge across sessions — the root cause of duplicate rows (e.g. 3 rows each for 'Lore temporal pruning' and 'Lore pruning defaults'). The guard is skipped when an explicit id is provided (cross-machine import via agents-file), since importFromFile handles dedup separately via UUID matching.
<!-- lore:019c904b-791e-772a-ab2b-93ac892a960c -->
* **buildSection() renders AGENTS.md directly without formatKnowledge**: In agents-file.ts, buildSection() now iterates DB entries directly grouped by category, emitting ### Category headings with \<!-- lore:UUID --> markers before each \* \*\*Title\*\*: Content bullet. This replaced the old approach of calling formatKnowledge() (remark AST serialization) then matching rendered bullets back to entries by title via a Map to inject UUID markers. The old title-keyed Map caused data loss when multiple entries shared the same title — Map.set() overwrote, giving all same-title bullets the same UUID. On re-import, seenIds skipped duplicates, silently losing entries. formatKnowledge() is now only used for system prompt injection where UUID tracking isn't needed. Individual entry content is serialized via remark for proper markdown escaping.
<!-- lore:019c8f8c-47c3-71a2-b5fd-248a2cfeba78 -->
* **Lore temporal pruning runs after distillation and curation on session.idle**: In src/index.ts, the session.idle handler now awaits both backgroundDistill and backgroundCurate (changed from fire-and-forget) before running temporal.prune(). This ordering is critical: pruning must run after both pipelines complete so it never deletes messages that haven't been processed. The prune call is wrapped in try/catch and logs when rows are deleted. Config comes from cfg.pruning.retention (days) and cfg.pruning.maxStorage (MB).
<!-- lore:019c8f4f-67c8-7cf4-b93b-c5ec46ed94b6 -->
* **Lore DB uses incremental auto\_vacuum to prevent free-page bloat**: The lore SQLite database (lore.db at ~/.local/share/opencode-lore/) accumulated 83% free pages from deletions — 428MB on disk with only 71MB of actual data. Fixed in schema version 3 migration: sets PRAGMA auto\_vacuum = INCREMENTAL then runs VACUUM. The migration is handled as a special case in migrate() since VACUUM can't run inside a transaction. The startup code also sets the pragma (no-op after migration, but ensures fresh installs get it). With incremental auto\_vacuum, freed pages return to the OS on each transaction commit instead of accumulating in the freelist. The temporal\_messages table (~51MB, ~6K rows) is the primary storage consumer; knowledge table is tiny (~74KB).
<!-- lore:20fe6f48-91de-4be3-bb29-a88e8238bd49 -->
* **AGENTS.md dedup via curator LLM at import time**: When multiple developers using lore merge branches, duplicate semantic entries can appear in AGENTS.md (different UUIDs, same meaning). Dedup is handled by the curator LLM during the import path — not as a separate compaction pass. The import sends parsed entries to the curator along with existing DB entries; the curator already has logic to produce update/delete ops when entries overlap semantically. This means dedup piggybacks on import, which runs at startup when the file has changed since last export. The import prompt is enhanced to explicitly ask: 'if any entries are semantically duplicative of existing entries, merge them.' For mangled \`\<!-- lore:UUID -->\` markers after merge conflict resolution: missing marker = treat as hand-written (new entry, curator handles dedup); duplicate same UUID = keep first, ignore second; malformed marker = ignore, treat as hand-written. Change detection uses content hash or mtime comparison of the lore section.
<!-- lore:019c8f95-c1a4-7b04-a575-23ebf89d14c3 -->
* **Lore gradient tryFit rebuilds message array from scratch every turn**: The gradient transform's tryFit() (gradient.ts) walks backwards from the end of messages, accumulating tokens until the raw budget is filled, then prepends distilled prefix messages. This produces a completely new array layout every turn — it doesn't track or preserve previous array positions. Combined with layer escalation (which strips tool outputs from previously-intact messages) and distillation updates (which change prefix content), the message array is never stable between consecutive turns at high utilization. This is the core incompatibility with prefix-based prompt caching. Critically, tryFit treats ALL messages as evictable, including messages from the current agentic turn. In tool-call loops, each step is a separate Message sharing the same parentID. As steps accumulate, their total tokens can push the cutoff forward past earlier steps in the same turn, causing the model to lose context of its own prior work. This is the root cause of infinite tool-call loops where the model repeats the same command. The fix requires protecting the current turn (last user message + all assistant messages sharing its parentID) as an atomic unit that tryFit never splits.
<!-- lore:019c8f95-c19d-7269-90b3-5a5b0a43ee45 -->
* **Lore cache-preserving fix: Approach A (defer) then Approach B (stable prefix)**: Implemented fix for gradient transform cache invalidation — three layers, now layers 1+2 are shipped: \*\*Layer 0 — Approach A (passthrough):\*\* transform() estimates total message tokens first. If they fit within \`usable\` budget, returns \`{ messages: input.messages, layer: 0 }\` — same array reference, completely untouched. No DB reads, no prefix building, no tryFit(). In index.ts, the transform hook skips splice() and trailing-message check when layer === 0. This preserves append-only message patterns for prompt caching (~97% cache efficiency). Raw messages are strictly better context than lossy distilled summaries. \*\*Layer 1 — Approach C (append-only prefix cache):\*\* When gradient mode activates (context exhausted), \`distilledPrefixCached()\` tracks lastDistillationID, rowCount, and cachedText per session. Three paths: (1) no new rows → returns exact same prefixMessages object (byte-identical for cache); (2) new rows appended → renders only delta via formatDistillations(), appends to cachedText; (3) session change or meta-distillation rewrote rows → full re-render. \`buildPrefixMessages()\` extracted from old \`distilledPrefix()\` so both cached and non-cached paths share the message-wrapping logic. Layer 1 uses cached path; layers 2-4 use non-cached \`distilledPrefix()\` since they already invalidate cache via tool stripping. \*\*Layer 2 — Approach B (lazy eviction, not yet implemented):\*\* Track previous raw window cutoff by message ID, only advance when budget truly exceeded. SafetyLayer type is now \`0 | 1 | 2 | 3 | 4\`. \`resetPrefixCache()\` exported for testing.

### Decision

<!-- lore:019c904b-7924-7187-8471-8ad2423b8946 -->
* **Curator prompt scoped to code-relevant knowledge only**: CURATOR\_SYSTEM in src/prompt.ts now explicitly excludes: general ecosystem knowledge available online, business strategy and marketing positioning, product pricing models, third-party tool details not needed for development, and personal contact information. This was added after the curator extracted entries about OpenWork integration strategy (including an email address), Lore Cloud pricing tiers, and AGENTS.md ecosystem facts — none of which help an agent write code. The curatorUser() function also appends guidance to prefer updating existing entries over creating new ones for the same concept, reducing duplicate creation.
<!-- lore:019c8f8c-47c6-7e5a-8e93-7721dc1378dc -->
* **Lore pruning defaults: 120-day retention, 1GB size cap**: User chose 120 days retention (not the originally proposed 7 days) and 1GB max storage (not 200MB). Rationale: the system is new and 7 days/200MB was based on only one week of data from a one-week-old system — not indicative of real growth. The generous defaults preserve recall capability and historical context. Config is in .lore.json under pruning.retention (days) and pruning.maxStorage (MB).
<!-- lore:dd60622e-6cf3-48c7-9715-f44fb054e150 -->
* **Use uuidv7 npm package for knowledge entry IDs**: User chose the \`uuidv7\` npm package (https://npmx.dev/package/uuidv7, by LiosK) over a self-contained ~15 line implementation. The package is RFC 9562 compliant, provides \`uuidv7()\` function that returns standard UUID string format, has a 42-bit counter for sub-millisecond monotonic ordering, and is clean/minimal. Usage: \`import { uuidv7 } from 'uuidv7'; const id = uuidv7();\` replaces \`crypto.randomUUID()\` in \`ltm.ts:31\`. Added as a runtime dependency in package.json.
<!-- End lore-managed section -->
